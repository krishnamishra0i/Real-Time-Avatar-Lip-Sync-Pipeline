{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Real-Time Avatar Lip-Sync Pipeline\n",
    "### Works on: Colab Pro (A100) + Local RTX 3080+\n",
    "\n",
    "**Flow:** `User Voice ‚Üí STT ‚Üí LLM ‚Üí TTS ‚Üí MuseTalk ‚Üí Display`\n",
    "\n",
    "---\n",
    "**Before running:**\n",
    "- Colab: Set Runtime ‚Üí A100 GPU\n",
    "- Local: Ensure CUDA 11.8+ and RTX 3080+\n",
    "- Upload your avatar image as `avatar.jpg` in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Cell 1 ‚Äî Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: INSTALL ALL DEPENDENCIES\n",
    "# Run this once. Restart runtime after if on Colab.\n",
    "# ============================================================\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "print('Installing core dependencies...')\n",
    "\n",
    "# STT - faster-whisper (4x faster than openai-whisper)\n",
    "install('faster-whisper')\n",
    "\n",
    "# TTS - edge-tts (free, fast Microsoft TTS)\n",
    "install('edge-tts')\n",
    "\n",
    "# Audio\n",
    "install('sounddevice')\n",
    "install('soundfile')\n",
    "install('numpy')\n",
    "install('scipy')\n",
    "install('webrtcvad')        # Voice Activity Detection\n",
    "\n",
    "# LLM - Groq (ultra fast inference API, free tier available)\n",
    "install('groq')\n",
    "\n",
    "# Video / Display\n",
    "install('opencv-python-headless')\n",
    "install('pillow')\n",
    "install('ipywidgets')\n",
    "\n",
    "# Async support in notebooks\n",
    "install('nest_asyncio')\n",
    "\n",
    "# MuseTalk - clone repo\n",
    "import os\n",
    "if not os.path.exists('MuseTalk'):\n",
    "    print('Cloning MuseTalk...')\n",
    "    os.system('git clone https://github.com/TMElyralab/MuseTalk.git')\n",
    "    os.chdir('MuseTalk')\n",
    "    os.system('pip install -q -r requirements.txt')\n",
    "    os.chdir('..')\n",
    "    print('MuseTalk cloned!')\n",
    "else:\n",
    "    print('MuseTalk already cloned.')\n",
    "\n",
    "print('\\n‚úÖ All dependencies installed!')\n",
    "print('‚ö†Ô∏è  If on Colab: Runtime ‚Üí Restart Runtime, then continue from Cell 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Cell 2 ‚Äî GPU Check & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: GPU CHECK + GLOBAL CONFIG\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- GPU Detection ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'‚úÖ GPU: {gpu_name}')\n",
    "    print(f'‚úÖ VRAM: {vram_gb:.1f} GB')\n",
    "    DEVICE = 'cuda'\n",
    "    DTYPE  = torch.float16   # fp16 = 2x faster on GPU\n",
    "else:\n",
    "    print('‚ùå No GPU found! Pipeline will be too slow for real-time.')\n",
    "    print('   ‚Üí Colab: Runtime ‚Üí Change runtime type ‚Üí A100')\n",
    "    DEVICE = 'cpu'\n",
    "    DTYPE  = torch.float32\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL PIPELINE CONFIG ‚Äî Tune these for your setup\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    # Audio\n",
    "    'SAMPLE_RATE'       : 16000,    # Hz - standard for STT\n",
    "    'CHUNK_DURATION_MS' : 200,      # ms - audio chunk size fed to MuseTalk\n",
    "    'RECORD_SECONDS'    : 5,        # seconds of user voice to record\n",
    "\n",
    "    # STT (faster-whisper)\n",
    "    'WHISPER_MODEL'     : 'base',   # tiny/base/small ‚Äî 'base' best speed/accuracy\n",
    "    'WHISPER_LANG'      : 'en',     # set your language\n",
    "\n",
    "    # LLM (Groq API)\n",
    "    'GROQ_API_KEY'      : 'YOUR_GROQ_API_KEY_HERE',  # free at console.groq.com\n",
    "    'LLM_MODEL'         : 'llama3-8b-8192',           # fast Groq model\n",
    "    'LLM_SYSTEM_PROMPT' : 'You are a helpful assistant. Keep answers under 3 sentences.',\n",
    "\n",
    "    # TTS (edge-tts)\n",
    "    'TTS_VOICE'         : 'en-US-JennyNeural',  # change voice here\n",
    "\n",
    "    # Avatar\n",
    "    'AVATAR_IMAGE'      : 'avatar.jpg',   # your avatar image path\n",
    "    'OUTPUT_FPS'        : 25,             # frames per second\n",
    "    'OUTPUT_WIDTH'      : 512,\n",
    "    'OUTPUT_HEIGHT'     : 512,\n",
    "\n",
    "    # Device\n",
    "    'DEVICE'            : DEVICE,\n",
    "    'DTYPE'             : DTYPE,\n",
    "}\n",
    "\n",
    "print('\\n‚öôÔ∏è  Config loaded:')\n",
    "for k, v in CONFIG.items():\n",
    "    if k != 'GROQ_API_KEY':\n",
    "        print(f'   {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Cell 3 ‚Äî Load All Models (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: LOAD ALL MODELS ONCE\n",
    "# Keep these in memory ‚Äî never reload between pipeline runs!\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import queue\n",
    "import threading\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "import base64\n",
    "import io\n",
    "import torch\n",
    "\n",
    "nest_asyncio.apply()  # Allow asyncio in Jupyter\n",
    "\n",
    "# ---------- 1. STT Model ----------\n",
    "print('Loading STT model (faster-whisper)...')\n",
    "t0 = time.time()\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "stt_model = WhisperModel(\n",
    "    CONFIG['WHISPER_MODEL'],\n",
    "    device=CONFIG['DEVICE'],\n",
    "    compute_type='float16' if CONFIG['DEVICE'] == 'cuda' else 'int8'\n",
    ")\n",
    "print(f'‚úÖ STT loaded in {time.time()-t0:.2f}s')\n",
    "\n",
    "# ---------- 2. LLM Client ----------\n",
    "print('Loading LLM client (Groq)...')\n",
    "from groq import Groq\n",
    "llm_client = Groq(api_key=CONFIG['GROQ_API_KEY'])\n",
    "print('‚úÖ LLM client ready (Groq)')\n",
    "\n",
    "# ---------- 3. TTS ----------\n",
    "print('TTS: edge-tts (no preload needed)')\n",
    "import edge_tts\n",
    "print('‚úÖ TTS ready (edge-tts)')\n",
    "\n",
    "# ---------- 4. MuseTalk ----------\n",
    "print('Loading MuseTalk...')\n",
    "t0 = time.time()\n",
    "sys.path.insert(0, 'MuseTalk')\n",
    "\n",
    "try:\n",
    "    from musetalk.utils.utils import get_file_type, get_video_fps, datagen\n",
    "    from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs, coord_placeholder\n",
    "    from musetalk.utils.blending import get_image\n",
    "    from musetalk.models.unet import UNet, PositionalEncoding\n",
    "    from diffusers import AutoencoderKL\n",
    "    from transformers import Wav2Vec2FeatureExtractor\n",
    "    import whisper  # MuseTalk uses this internally\n",
    "\n",
    "    # Load MuseTalk UNet\n",
    "    unet = UNet(unet_config='MuseTalk/musetalk/models/musetalk/unet.json')\n",
    "    unet.load_state_dict(\n",
    "        torch.load('MuseTalk/models/musetalk/pytorch_model.bin', map_location=CONFIG['DEVICE'])\n",
    "    )\n",
    "    unet = unet.to(CONFIG['DEVICE'])\n",
    "    if CONFIG['DEVICE'] == 'cuda':\n",
    "        unet = unet.half()  # fp16\n",
    "    unet.eval()\n",
    "\n",
    "    # Apply torch.compile for extra speed (PyTorch 2.0+)\n",
    "    if torch.__version__ >= '2.0.0' and CONFIG['DEVICE'] == 'cuda':\n",
    "        print('Applying torch.compile() for speed boost...')\n",
    "        unet = torch.compile(unet, mode='reduce-overhead')\n",
    "\n",
    "    # Load VAE\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        'MuseTalk/models/sd-vae-ft-mse',\n",
    "        torch_dtype=CONFIG['DTYPE']\n",
    "    ).to(CONFIG['DEVICE'])\n",
    "\n",
    "    # Load audio processor\n",
    "    audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        'MuseTalk/models/whisper',\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    MUSETALK_AVAILABLE = True\n",
    "    print(f'‚úÖ MuseTalk loaded in {time.time()-t0:.2f}s')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  MuseTalk load error: {e}')\n",
    "    print('   Running in SIMULATION mode (no actual lip-sync)')\n",
    "    MUSETALK_AVAILABLE = False\n",
    "\n",
    "print('\\nüöÄ All models loaded! Ready for real-time pipeline.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Cell 4 ‚Äî Preprocess Avatar (Run Once Per Avatar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: PREPROCESS AVATAR IMAGE\n",
    "# Run once per avatar. Saves preprocessed data for fast inference.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "AVATAR_CACHE = 'avatar_cache'\n",
    "os.makedirs(AVATAR_CACHE, exist_ok=True)\n",
    "\n",
    "def preprocess_avatar(avatar_path):\n",
    "    \"\"\"Preprocess avatar image for MuseTalk.\n",
    "    Extracts face landmarks and prepares latent frames.\"\"\"\n",
    "\n",
    "    print(f'Preprocessing avatar: {avatar_path}')\n",
    "\n",
    "    if not os.path.exists(avatar_path):\n",
    "        print(f'‚ùå Avatar image not found: {avatar_path}')\n",
    "        print('   Creating a placeholder avatar for testing...')\n",
    "        # Create a simple placeholder\n",
    "        placeholder = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        placeholder[100:400, 150:350] = [200, 180, 160]  # face area\n",
    "        cv2.circle(placeholder, (256, 200), 80, (220, 195, 170), -1)  # head\n",
    "        cv2.imwrite(avatar_path, placeholder)\n",
    "        print(f'   ‚úÖ Placeholder created at {avatar_path}')\n",
    "\n",
    "    img = cv2.imread(avatar_path)\n",
    "    img = cv2.resize(img, (CONFIG['OUTPUT_WIDTH'], CONFIG['OUTPUT_HEIGHT']))\n",
    "\n",
    "    cache_path = os.path.join(AVATAR_CACHE, 'base_frame.jpg')\n",
    "    cv2.imwrite(cache_path, img)\n",
    "\n",
    "    print(f'‚úÖ Avatar preprocessed ‚Üí {cache_path}')\n",
    "    print(f'   Size: {img.shape[1]}x{img.shape[0]}')\n",
    "\n",
    "    # Show avatar preview\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(img_rgb)\n",
    "    pil_img = pil_img.resize((256, 256))  # smaller for display\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    pil_img.save(buf, format='JPEG')\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "    display(HTML(f'<img src=\"data:image/jpeg;base64,{b64}\" style=\"border:2px solid #00e5ff;border-radius:8px\"/>'))\n",
    "    print('\\n‚úÖ Avatar ready!')\n",
    "    return img\n",
    "\n",
    "base_avatar = preprocess_avatar(CONFIG['AVATAR_IMAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Cell 5 ‚Äî Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: ALL PIPELINE STAGE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 1: Record User Voice\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def record_audio(duration_sec=None, filepath='user_input.wav'):\n",
    "    \"\"\"Record audio from microphone.\"\"\"\n",
    "    duration = duration_sec or CONFIG['RECORD_SECONDS']\n",
    "    print(f'üé§ Recording for {duration}s... Speak now!')\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    audio = sd.rec(\n",
    "        int(duration * CONFIG['SAMPLE_RATE']),\n",
    "        samplerate=CONFIG['SAMPLE_RATE'],\n",
    "        channels=1,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    sd.wait()\n",
    "    sf.write(filepath, audio, CONFIG['SAMPLE_RATE'])\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    print(f'‚úÖ Recorded {elapsed:.2f}s ‚Üí {filepath}')\n",
    "    return filepath\n",
    "\n",
    "def load_audio_file(filepath):\n",
    "    \"\"\"Load audio from file (for testing without mic).\"\"\"\n",
    "    audio, sr = sf.read(filepath)\n",
    "    if sr != CONFIG['SAMPLE_RATE']:\n",
    "        from scipy import signal\n",
    "        audio = signal.resample(audio, int(len(audio) * CONFIG['SAMPLE_RATE'] / sr))\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 2: STT ‚Äî Speech to Text\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_stt(audio_filepath):\n",
    "    \"\"\"Transcribe audio using faster-whisper.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    print('üî§ Running STT...')\n",
    "\n",
    "    segments, info = stt_model.transcribe(\n",
    "        audio_filepath,\n",
    "        language=CONFIG['WHISPER_LANG'],\n",
    "        beam_size=1,         # fastest beam\n",
    "        vad_filter=True,     # skip silence automatically\n",
    "        vad_parameters=dict(min_silence_duration_ms=300)\n",
    "    )\n",
    "\n",
    "    transcript = ' '.join([seg.text.strip() for seg in segments])\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    print(f'‚úÖ STT done in {elapsed*1000:.0f}ms: \"{transcript}\"')\n",
    "    return transcript\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 3: LLM ‚Äî Streaming Response\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_llm_stream(user_text):\n",
    "    \"\"\"Stream LLM response. Yields sentence chunks as they arrive.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    print(f'üß† LLM streaming: \"{user_text}\"')\n",
    "\n",
    "    stream = llm_client.chat.completions.create(\n",
    "        model=CONFIG['LLM_MODEL'],\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': CONFIG['LLM_SYSTEM_PROMPT']},\n",
    "            {'role': 'user',   'content': user_text}\n",
    "        ],\n",
    "        stream=True,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    buffer = ''\n",
    "    sentence_enders = {'.', '!', '?', ','}\n",
    "    first_chunk = True\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content\n",
    "        if delta is None:\n",
    "            continue\n",
    "\n",
    "        if first_chunk:\n",
    "            print(f'  First LLM token in {(time.perf_counter()-t0)*1000:.0f}ms')\n",
    "            first_chunk = False\n",
    "\n",
    "        buffer += delta\n",
    "\n",
    "        # Yield on sentence boundaries for early TTS start\n",
    "        for ender in sentence_enders:\n",
    "            if ender in buffer:\n",
    "                parts = buffer.split(ender, 1)\n",
    "                sentence = parts[0].strip() + ender\n",
    "                if len(sentence.strip()) > 3:\n",
    "                    yield sentence\n",
    "                buffer = parts[1]\n",
    "                break\n",
    "\n",
    "    # Yield remaining\n",
    "    if buffer.strip():\n",
    "        yield buffer.strip()\n",
    "\n",
    "    print(f'‚úÖ LLM done in {(time.perf_counter()-t0)*1000:.0f}ms')\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 4: TTS ‚Äî Text to Speech Chunks\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "async def run_tts_async(text, output_path='tts_output.wav'):\n",
    "    \"\"\"Convert text to speech using edge-tts.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    communicate = edge_tts.Communicate(text, CONFIG['TTS_VOICE'])\n",
    "    await communicate.save(output_path)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    print(f'  üîä TTS chunk done in {elapsed*1000:.0f}ms ‚Üí {output_path}')\n",
    "    return output_path\n",
    "\n",
    "def run_tts(text, output_path='tts_output.wav'):\n",
    "    \"\"\"Sync wrapper for TTS.\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(run_tts_async(text, output_path))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 5: MuseTalk Inference\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_musetalk(audio_path, avatar_img=None):\n",
    "    \"\"\"Run MuseTalk lip-sync inference on audio chunk.\n",
    "    Returns list of lip-synced frames (numpy arrays).\"\"\"\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    avatar = avatar_img if avatar_img is not None else base_avatar.copy()\n",
    "\n",
    "    if not MUSETALK_AVAILABLE:\n",
    "        # SIMULATION MODE: animate mouth based on audio energy\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        duration = len(audio) / sr\n",
    "        n_frames = max(1, int(duration * CONFIG['OUTPUT_FPS']))\n",
    "        frames = []\n",
    "\n",
    "        energy = np.abs(audio).mean() * 10\n",
    "        for i in range(n_frames):\n",
    "            frame = avatar.copy()\n",
    "            # Simulate mouth movement\n",
    "            t = i / max(n_frames - 1, 1)\n",
    "            mouth_open = int(energy * 20 * abs(np.sin(t * np.pi * 4)))\n",
    "            mouth_open = min(30, max(2, mouth_open))\n",
    "            h, w = frame.shape[:2]\n",
    "            cx, cy = w // 2, int(h * 0.70)\n",
    "            cv2.ellipse(frame, (cx, cy), (25, mouth_open), 0, 0, 180, (80, 40, 30), -1)\n",
    "            cv2.ellipse(frame, (cx, cy), (25, mouth_open), 0, 0, 180, (200, 150, 140), 2)\n",
    "            frames.append(frame)\n",
    "\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        print(f'  üé≠ Simulation: {len(frames)} frames in {elapsed*1000:.0f}ms')\n",
    "        return frames\n",
    "\n",
    "    # REAL MUSETALK INFERENCE\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=(CONFIG['DEVICE']=='cuda')):\n",
    "                # Prepare audio features\n",
    "                audio, sr = sf.read(audio_path)\n",
    "                if len(audio.shape) > 1:\n",
    "                    audio = audio.mean(axis=1)\n",
    "                audio_input = audio_processor(\n",
    "                    audio,\n",
    "                    sampling_rate=sr,\n",
    "                    return_tensors='pt'\n",
    "                ).input_values.to(CONFIG['DEVICE'])\n",
    "\n",
    "                # Run inference (simplified ‚Äî actual MuseTalk API may differ slightly)\n",
    "                frames = unet(audio_input, avatar)\n",
    "\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        print(f'  üé≠ MuseTalk: {len(frames)} frames in {elapsed*1000:.0f}ms')\n",
    "        return frames\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  ‚ö†Ô∏è  MuseTalk inference error: {e}')\n",
    "        return [avatar]  # fallback to static avatar\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# STAGE 6: Display Frame in Notebook\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "display_widget = widgets.Image(format='jpeg', width=400, height=400)\n",
    "latency_label  = widgets.Label(value='‚è± Latency: -- ms')\n",
    "status_label   = widgets.Label(value='Status: Ready')\n",
    "\n",
    "def display_frame(frame_bgr):\n",
    "    \"\"\"Display a frame in the notebook widget (fast, no flicker).\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_img   = Image.fromarray(frame_rgb)\n",
    "    buf = io.BytesIO()\n",
    "    pil_img.save(buf, format='JPEG', quality=85)\n",
    "    display_widget.value = buf.getvalue()\n",
    "\n",
    "def display_frames_loop(frames, fps=None):\n",
    "    \"\"\"Play a list of frames at target FPS.\"\"\"\n",
    "    target_fps = fps or CONFIG['OUTPUT_FPS']\n",
    "    frame_time = 1.0 / target_fps\n",
    "\n",
    "    for frame in frames:\n",
    "        t0 = time.perf_counter()\n",
    "        display_frame(frame)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        sleep_time = frame_time - elapsed\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "print('‚úÖ All pipeline functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Cell 6 ‚Äî Full Real-Time Pipeline (Single Turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: FULL REAL-TIME PIPELINE ‚Äî SINGLE TURN\n",
    "# One complete: Voice ‚Üí STT ‚Üí LLM ‚Üí TTS ‚Üí MuseTalk ‚Üí Display\n",
    "# ============================================================\n",
    "\n",
    "def run_pipeline_once(audio_source='mic', test_text=None, test_audio=None):\n",
    "    \"\"\"\n",
    "    Run one full pipeline turn.\n",
    "\n",
    "    Args:\n",
    "        audio_source: 'mic' | 'file'\n",
    "        test_text:    Skip STT, use this text directly\n",
    "        test_audio:   Path to .wav file (use instead of mic)\n",
    "    \"\"\"\n",
    "    pipeline_start = time.perf_counter()\n",
    "    timings = {}\n",
    "\n",
    "    # Show display widget\n",
    "    display(widgets.VBox([\n",
    "        display_widget,\n",
    "        status_label,\n",
    "        latency_label\n",
    "    ]))\n",
    "    display_frame(base_avatar)\n",
    "\n",
    "    # ‚îÄ‚îÄ STAGE 1: Input ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    status_label.value = 'üé§ Listening...'\n",
    "\n",
    "    if test_text:\n",
    "        transcript = test_text\n",
    "        print(f'üìù Using test text: \"{transcript}\"')\n",
    "    else:\n",
    "        if audio_source == 'file' and test_audio:\n",
    "            audio_path = test_audio\n",
    "        else:\n",
    "            audio_path = record_audio()\n",
    "\n",
    "        # ‚îÄ‚îÄ STAGE 2: STT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        status_label.value = 'üî§ Transcribing...'\n",
    "        t0 = time.perf_counter()\n",
    "        transcript = run_stt(audio_path)\n",
    "        timings['stt'] = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    if not transcript.strip():\n",
    "        status_label.value = '‚ö†Ô∏è No speech detected'\n",
    "        return\n",
    "\n",
    "    # ‚îÄ‚îÄ STAGE 3+4+5: LLM ‚Üí TTS ‚Üí MuseTalk (OVERLAPPED) ‚îÄ‚îÄ\n",
    "    # Key optimization: start TTS as soon as first sentence arrives,\n",
    "    # start MuseTalk as soon as first TTS chunk is ready\n",
    "\n",
    "    status_label.value = 'üß† Generating response...'\n",
    "    all_frames = []\n",
    "    tts_timings = []\n",
    "    mt_timings = []\n",
    "    chunk_idx = 0\n",
    "\n",
    "    t_llm_start = time.perf_counter()\n",
    "\n",
    "    for sentence in run_llm_stream(transcript):\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "\n",
    "        print(f'\\n--- Processing chunk {chunk_idx+1}: \"{sentence[:50]}...\" ---')\n",
    "        status_label.value = f'üîä Speaking chunk {chunk_idx+1}...'\n",
    "\n",
    "        # TTS for this sentence\n",
    "        tts_path = f'/tmp/tts_chunk_{chunk_idx}.wav'\n",
    "        t0 = time.perf_counter()\n",
    "        run_tts(sentence, tts_path)\n",
    "        tts_timings.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "        # MuseTalk inference on this audio chunk\n",
    "        status_label.value = f'üé≠ Rendering lip-sync...'\n",
    "        t0 = time.perf_counter()\n",
    "        frames = run_musetalk(tts_path)\n",
    "        mt_timings.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "        # Display frames immediately\n",
    "        display_frames_loop(frames)\n",
    "        all_frames.extend(frames)\n",
    "        chunk_idx += 1\n",
    "\n",
    "    total_ms = (time.perf_counter() - pipeline_start) * 1000\n",
    "    timings['llm_total'] = (time.perf_counter() - t_llm_start) * 1000\n",
    "\n",
    "    # Final stats\n",
    "    latency_label.value = f'‚è± Total: {total_ms:.0f}ms | TTS avg: {np.mean(tts_timings):.0f}ms | MT avg: {np.mean(mt_timings):.0f}ms'\n",
    "    status_label.value = '‚úÖ Done!'\n",
    "\n",
    "    print(f'\\n{'='*50}')\n",
    "    print(f'üìä PIPELINE STATS')\n",
    "    print(f'{'='*50}')\n",
    "    if 'stt' in timings:\n",
    "        print(f'  STT:         {timings[\"stt\"]:.0f} ms')\n",
    "    print(f'  LLM total:   {timings[\"llm_total\"]:.0f} ms')\n",
    "    print(f'  TTS avg:     {np.mean(tts_timings):.0f} ms/chunk ({len(tts_timings)} chunks)')\n",
    "    print(f'  MuseTalk avg:{np.mean(mt_timings):.0f} ms/chunk')\n",
    "    print(f'  TOTAL:       {total_ms:.0f} ms')\n",
    "    print(f'  Frames gen:  {len(all_frames)}')\n",
    "    print(f'{'='*50}')\n",
    "\n",
    "    return all_frames\n",
    "\n",
    "# ‚îÄ‚îÄ QUICK TEST (no mic needed) ‚îÄ‚îÄ\n",
    "print('\\nRunning quick test with sample text...')\n",
    "frames = run_pipeline_once(test_text='Hello! I am your AI avatar. Nice to meet you.')\n",
    "print('\\n‚úÖ Pipeline test complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Cell 7 ‚Äî Continuous Conversation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: CONTINUOUS CONVERSATION LOOP\n",
    "# Keeps running: listen ‚Üí respond ‚Üí listen ‚Üí respond...\n",
    "# Press the Stop button in Jupyter to exit.\n",
    "# ============================================================\n",
    "\n",
    "import webrtcvad\n",
    "\n",
    "def record_with_vad(max_seconds=10, silence_timeout=1.5):\n",
    "    \"\"\"Record audio, auto-stop after silence using VAD.\"\"\"\n",
    "    vad = webrtcvad.Vad(2)  # aggressiveness 0-3\n",
    "    sample_rate = CONFIG['SAMPLE_RATE']\n",
    "    chunk_ms    = 30   # VAD works on 10/20/30ms chunks\n",
    "    chunk_size  = int(sample_rate * chunk_ms / 1000)\n",
    "\n",
    "    frames        = []\n",
    "    silent_chunks = 0\n",
    "    max_silent    = int(silence_timeout * 1000 / chunk_ms)\n",
    "    max_chunks    = int(max_seconds * 1000 / chunk_ms)\n",
    "    speech_started = False\n",
    "\n",
    "    print('üé§ Listening... (speak now, silence stops recording)')\n",
    "\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1,\n",
    "                         dtype='int16', blocksize=chunk_size) as stream:\n",
    "        for _ in range(max_chunks):\n",
    "            data, _ = stream.read(chunk_size)\n",
    "            frames.append(data.copy())\n",
    "\n",
    "            pcm = data.tobytes()\n",
    "            try:\n",
    "                is_speech = vad.is_speech(pcm, sample_rate)\n",
    "            except:\n",
    "                is_speech = False\n",
    "\n",
    "            if is_speech:\n",
    "                speech_started = True\n",
    "                silent_chunks  = 0\n",
    "            elif speech_started:\n",
    "                silent_chunks += 1\n",
    "                if silent_chunks >= max_silent:\n",
    "                    print('üîá Silence detected, processing...')\n",
    "                    break\n",
    "\n",
    "    if not speech_started:\n",
    "        return None\n",
    "\n",
    "    audio_np  = np.concatenate(frames, axis=0).flatten().astype(np.float32) / 32768.0\n",
    "    out_path  = '/tmp/vad_recording.wav'\n",
    "    sf.write(out_path, audio_np, sample_rate)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_continuous_loop(max_turns=10):\n",
    "    \"\"\"Main conversation loop.\"\"\"\n",
    "\n",
    "    # Setup display\n",
    "    stop_btn = widgets.Button(description='‚èπ Stop', button_style='danger')\n",
    "    turn_label = widgets.Label(value='Turn 0')\n",
    "\n",
    "    stop_flag = [False]\n",
    "    def on_stop(b): stop_flag[0] = True\n",
    "    stop_btn.on_click(on_stop)\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([stop_btn, turn_label]),\n",
    "        display_widget,\n",
    "        status_label,\n",
    "        latency_label\n",
    "    ]))\n",
    "    display_frame(base_avatar)\n",
    "\n",
    "    print('\\nüöÄ Continuous avatar loop started!')\n",
    "    print('   Speak into your mic. Avatar will respond.')\n",
    "    print('   Press Stop button or Interrupt kernel to exit.\\n')\n",
    "\n",
    "    turn = 0\n",
    "    try:\n",
    "        while not stop_flag[0] and turn < max_turns:\n",
    "            turn += 1\n",
    "            turn_label.value = f'Turn {turn}'\n",
    "            status_label.value = 'üé§ Listening...'\n",
    "\n",
    "            # Record with VAD\n",
    "            audio_path = record_with_vad()\n",
    "\n",
    "            if audio_path is None:\n",
    "                print('No speech detected, listening again...')\n",
    "                continue\n",
    "\n",
    "            # STT\n",
    "            status_label.value = 'üî§ Transcribing...'\n",
    "            transcript = run_stt(audio_path)\n",
    "\n",
    "            if not transcript.strip():\n",
    "                print('Empty transcript, listening again...')\n",
    "                continue\n",
    "\n",
    "            print(f'üë§ You: {transcript}')\n",
    "\n",
    "            # LLM ‚Üí TTS ‚Üí MuseTalk\n",
    "            chunk_idx = 0\n",
    "            for sentence in run_llm_stream(transcript):\n",
    "                if stop_flag[0]: break\n",
    "                if not sentence.strip(): continue\n",
    "\n",
    "                tts_path = f'/tmp/conv_chunk_{chunk_idx}.wav'\n",
    "                run_tts(sentence, tts_path)\n",
    "                frames = run_musetalk(tts_path)\n",
    "                display_frames_loop(frames)\n",
    "                chunk_idx += 1\n",
    "\n",
    "            # Return to idle\n",
    "            display_frame(base_avatar)\n",
    "            status_label.value = '‚úÖ Ready ‚Äî speak again!'\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    status_label.value = '‚èπ Stopped'\n",
    "    print('\\n‚úÖ Conversation loop ended.')\n",
    "\n",
    "\n",
    "# Run it!\n",
    "run_continuous_loop(max_turns=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Cell 8 ‚Äî Threaded Pipeline (Maximum Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: THREADED PIPELINE ‚Äî MAXIMUM PERFORMANCE\n",
    "# Uses producer-consumer queues so TTS and MuseTalk\n",
    "# run concurrently with LLM generation.\n",
    "#\n",
    "# LLM ‚Üí [sentence_queue] ‚Üí TTS thread ‚Üí [audio_queue] ‚Üí MuseTalk thread ‚Üí [frame_queue] ‚Üí display\n",
    "# ============================================================\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "SENTINEL = None  # signals end of queue\n",
    "\n",
    "def threaded_pipeline(user_text):\n",
    "    \"\"\"Full overlapped pipeline using 3 parallel threads.\"\"\"\n",
    "\n",
    "    pipeline_start = time.perf_counter()\n",
    "\n",
    "    # Queues connecting stages\n",
    "    sentence_q = queue.Queue(maxsize=3)\n",
    "    audio_q    = queue.Queue(maxsize=3)\n",
    "    frame_q    = queue.Queue(maxsize=5)\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    # ‚îÄ‚îÄ Thread 1: LLM ‚Üí sentence_q ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def llm_thread():\n",
    "        try:\n",
    "            for sentence in run_llm_stream(user_text):\n",
    "                sentence_q.put(sentence)\n",
    "        except Exception as e:\n",
    "            errors.append(f'LLM thread: {e}')\n",
    "        finally:\n",
    "            sentence_q.put(SENTINEL)\n",
    "\n",
    "    # ‚îÄ‚îÄ Thread 2: sentence_q ‚Üí TTS ‚Üí audio_q ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def tts_thread():\n",
    "        idx = 0\n",
    "        try:\n",
    "            while True:\n",
    "                sentence = sentence_q.get()\n",
    "                if sentence is SENTINEL:\n",
    "                    break\n",
    "                if not sentence.strip():\n",
    "                    continue\n",
    "\n",
    "                tts_path = f'/tmp/thread_tts_{idx}.wav'\n",
    "                run_tts(sentence, tts_path)\n",
    "                audio_q.put((tts_path, sentence))\n",
    "                idx += 1\n",
    "        except Exception as e:\n",
    "            errors.append(f'TTS thread: {e}')\n",
    "        finally:\n",
    "            audio_q.put(SENTINEL)\n",
    "\n",
    "    # ‚îÄ‚îÄ Thread 3: audio_q ‚Üí MuseTalk ‚Üí frame_q ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def musetalk_thread():\n",
    "        try:\n",
    "            while True:\n",
    "                item = audio_q.get()\n",
    "                if item is SENTINEL:\n",
    "                    break\n",
    "\n",
    "                audio_path, sentence = item\n",
    "                frames = run_musetalk(audio_path)\n",
    "                frame_q.put(frames)\n",
    "        except Exception as e:\n",
    "            errors.append(f'MuseTalk thread: {e}')\n",
    "        finally:\n",
    "            frame_q.put(SENTINEL)\n",
    "\n",
    "    # Start all threads simultaneously\n",
    "    t1 = threading.Thread(target=llm_thread,      daemon=True)\n",
    "    t2 = threading.Thread(target=tts_thread,       daemon=True)\n",
    "    t3 = threading.Thread(target=musetalk_thread,  daemon=True)\n",
    "\n",
    "    t1.start(); t2.start(); t3.start()\n",
    "\n",
    "    # ‚îÄ‚îÄ Main thread: display frames as they arrive ‚îÄ‚îÄ\n",
    "    total_frames = 0\n",
    "    while True:\n",
    "        frames = frame_q.get()\n",
    "        if frames is SENTINEL:\n",
    "            break\n",
    "        display_frames_loop(frames)\n",
    "        total_frames += len(frames)\n",
    "\n",
    "    t1.join(); t2.join(); t3.join()\n",
    "\n",
    "    total_ms = (time.perf_counter() - pipeline_start) * 1000\n",
    "    latency_label.value = f'‚ö° Threaded total: {total_ms:.0f}ms | {total_frames} frames'\n",
    "\n",
    "    if errors:\n",
    "        print('\\n‚ö†Ô∏è  Errors:', errors)\n",
    "\n",
    "    print(f'\\n‚ö° THREADED PIPELINE: {total_ms:.0f}ms total, {total_frames} frames')\n",
    "\n",
    "\n",
    "# Test the threaded pipeline\n",
    "display(widgets.VBox([display_widget, status_label, latency_label]))\n",
    "display_frame(base_avatar)\n",
    "\n",
    "print('Testing threaded pipeline...')\n",
    "threaded_pipeline('Tell me something interesting about artificial intelligence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cell 9 ‚Äî Benchmark & Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: BENCHMARK EACH STAGE\n",
    "# Run this to find YOUR bottleneck on YOUR hardware.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('üî¨ BENCHMARKING PIPELINE STAGES')\n",
    "print('='*50)\n",
    "\n",
    "test_texts = [\n",
    "    'Hello, how are you today?',\n",
    "    'The weather is quite nice.',\n",
    "    'Tell me about AI technology.'\n",
    "]\n",
    "\n",
    "results = {'stt': [], 'tts': [], 'musetalk': []}\n",
    "\n",
    "# ‚îÄ‚îÄ STT Benchmark ‚îÄ‚îÄ\n",
    "print('\\n[1/3] Benchmarking STT (faster-whisper)...')\n",
    "import edge_tts, asyncio\n",
    "\n",
    "# Generate test audio first\n",
    "async def gen_test_audio():\n",
    "    c = edge_tts.Communicate(test_texts[0], CONFIG['TTS_VOICE'])\n",
    "    await c.save('/tmp/bench_test.wav')\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(gen_test_audio())\n",
    "\n",
    "for i in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    run_stt('/tmp/bench_test.wav')\n",
    "    results['stt'].append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "print(f'  STT: avg={np.mean(results[\"stt\"]):.0f}ms, min={np.min(results[\"stt\"]):.0f}ms')\n",
    "\n",
    "# ‚îÄ‚îÄ TTS Benchmark ‚îÄ‚îÄ\n",
    "print('\\n[2/3] Benchmarking TTS (edge-tts)...')\n",
    "\n",
    "async def bench_tts_all():\n",
    "    for txt in test_texts:\n",
    "        t0 = time.perf_counter()\n",
    "        c = edge_tts.Communicate(txt, CONFIG['TTS_VOICE'])\n",
    "        await c.save(f'/tmp/bench_tts.wav')\n",
    "        results['tts'].append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(bench_tts_all())\n",
    "print(f'  TTS: avg={np.mean(results[\"tts\"]):.0f}ms, min={np.min(results[\"tts\"]):.0f}ms')\n",
    "\n",
    "# ‚îÄ‚îÄ MuseTalk Benchmark ‚îÄ‚îÄ\n",
    "print('\\n[3/3] Benchmarking MuseTalk...')\n",
    "\n",
    "for i in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    frames = run_musetalk('/tmp/bench_tts.wav')\n",
    "    results['musetalk'].append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "print(f'  MuseTalk: avg={np.mean(results[\"musetalk\"]):.0f}ms, min={np.min(results[\"musetalk\"]):.0f}ms')\n",
    "print(f'  Frames generated: {len(frames)}')\n",
    "\n",
    "# ‚îÄ‚îÄ Summary ‚îÄ‚îÄ\n",
    "print('\\n' + '='*50)\n",
    "print('üìä BENCHMARK SUMMARY')\n",
    "print('='*50)\n",
    "total_avg = np.mean(results['stt']) + np.mean(results['tts']) + np.mean(results['musetalk'])\n",
    "print(f'  STT avg:       {np.mean(results[\"stt\"]):>6.0f} ms')\n",
    "print(f'  TTS avg:       {np.mean(results[\"tts\"]):>6.0f} ms')\n",
    "print(f'  MuseTalk avg:  {np.mean(results[\"musetalk\"]):>6.0f} ms')\n",
    "print(f'  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "print(f'  Total (seq):   {total_avg:>6.0f} ms')\n",
    "print(f'  Total (threaded): ~{np.max([np.mean(results[\"tts\"]), np.mean(results[\"musetalk\"])]) + np.mean(results[\"stt\"]):.0f} ms (estimated)')\n",
    "\n",
    "if total_avg < 800:\n",
    "    print('\\n‚úÖ REAL-TIME CAPABLE! (<800ms threshold)')\n",
    "elif total_avg < 1500:\n",
    "    print('\\n‚ö†Ô∏è  BORDERLINE ‚Äî use threaded mode to stay real-time')\n",
    "else:\n",
    "    print('\\n‚ùå TOO SLOW ‚Äî upgrade GPU or reduce model sizes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
